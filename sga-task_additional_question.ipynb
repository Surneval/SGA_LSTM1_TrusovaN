{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional question from tutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Given a map-reduce sequence of tasks, what would be the algorithm to convert it into Spark, and can one improve it in speed?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. How to convert a MapReduce sequence of tasks into Spark**\n",
    "\n",
    "1) **Understanding the MapReduce workflow:**\n",
    "\n",
    "* **Map phase:**\n",
    "\n",
    "Processes input data to produce intermediate key-value pairs.\n",
    "* **Reduce phase:**\n",
    "\n",
    "Aggregates the intermediate data based on keys to produce the final output.\n",
    "\n",
    "2) **Converting MapReduce tasks into Spark**\n",
    "\n",
    "**A. Map phase conversion:**\n",
    "\n",
    "- **In MapReduce:** each mapper processes a subset of the data, emitting key-value pairs.\n",
    "- **In Spark:** The `map()` and `mapValues()` transformations are used to process and transform the data.\n",
    "- **In the code:**\n",
    "  - **Parsing and initial mapping:** Each line is parsed into a structured record.\n",
    "    ```python\n",
    "    parsed_rdd = clickstream_rdd.map(parse_csv_line).filter(lambda x: x is not None)\n",
    "    ```\n",
    "  - **Key-Value mapping:** Maps each record to a key-value pair where the key is `(user_id, session_id)`.\n",
    "    ```python\n",
    "    grouped_rdd = parsed_rdd.map(lambda row: ((row['user_id'], row['session_id']), row))\n",
    "    ```\n",
    "\n",
    "\n",
    "**B. Shuffle and grouping:**\n",
    "\n",
    "- **In MapReduce:** Intermediate key-value pairs are shuffled and sorted to group values by key.\n",
    "- **In Spark:** The `groupByKey()` transformation groups all the values with the same key.\n",
    "- **In the code:** Groups events by `(user_id, session_id)`.\n",
    "  ```python\n",
    "  grouped_rdd = grouped_rdd.groupByKey()\n",
    "  ```\n",
    "\n",
    "**C. Reduce phase conversion:**\n",
    "\n",
    "- **In MapReduce:** Reducers aggregate the values for each key to produce the final output.\n",
    "- **In Spark:** Use transformations like `reduceByKey()` or `aggregateByKey()` for aggregation.\n",
    "- **In the Code:**\n",
    "  - **Constructing routes (reduction within each group):** Constructs the route for each user session.\n",
    "    ```python\n",
    "    routes_rdd = grouped_rdd.mapValues(construct_route)\n",
    "    ```\n",
    "  - **Mapping routes to counts:** Prepares data for counting occurrences of each route.\n",
    "    ```python\n",
    "    route_counts_rdd = routes_rdd.map(lambda x: (x[1], 1))\n",
    "    ```\n",
    "  - **Reducing by route key:** Aggregates the counts for each route.\n",
    "    ```python\n",
    "    final_counts_rdd = route_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    ```\n",
    "    \n",
    "\n",
    "**D. Collecting and sorting results:**\n",
    "\n",
    "- **In MapReduce:** Final results are written to HDFS or collected for further processing.\n",
    "- **In Spark:** Actions like `collect()`, `take()`, or `takeOrdered()` are used to retrieve results.\n",
    "- **In the code:** Retrieves the top 10 routes by count.\n",
    "  ```python\n",
    "  top_routes = final_counts_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
    "  ```\n",
    " \n",
    "\n",
    "**E. Key differences and advantages:**\n",
    "\n",
    "- **In-Memory computation:** Spark processes data in memory, reducing disk I/O compared to MapReduce.\n",
    "- **Lazy evaluation and DAG optimization:** Spark builds a Directed Acyclic Graph (DAG) of transformations, optimizing execution.\n",
    "- **Higher-level APIs:** Spark provides RDDs, DataFrames, and Datasets for more efficient data processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Can one improve it in speed?**\n",
    "\n",
    "Yes, the speed of the Spark job can be improved. Here are several ways to enhance performance:\n",
    "\n",
    "**A. Use DataFrames instead of RDDs**\n",
    "\n",
    "- **Why:** DataFrames are optimized through Spark's Catalyst optimizer and can offer significant performance improvements over RDDs.\n",
    "- **How to implement:** Convert data processing logic to use DataFrames and built-in functions.\n",
    "- **Benefits:**\n",
    "  - Better performance due to optimized execution plans.\n",
    "  - Easier to write and maintain code.\n",
    "\n",
    "\n",
    "**B. Avoid using UDFs when possible**\n",
    "\n",
    "- **Issue:** User-Defined Functions (UDFs) can be slower because they operate outside of Spark's optimization scope.\n",
    "- **Solution:** Use Spark SQL built-in functions if possible.\n",
    "- **Applicability:** In this case, since route construction involves custom logic, using a UDF is acceptable. However, ensure the UDF is efficient.\n",
    "\n",
    "**C. Reduce data shuffling**\n",
    "\n",
    "- **Optimization:** Use `reduceByKey()` instead of `groupByKey()` as it performs local aggregation before shuffling data across the network.\n",
    "\n",
    "\n",
    "**D. Cache intermediate RDDs**\n",
    "\n",
    "- **Benefit:** If an RDD or DataFrame is reused multiple times, caching it can prevent recomputation.\n",
    "- **Implementation:**\n",
    "  ```python\n",
    "  routes_rdd.persist()\n",
    "  ```\n",
    "\n",
    "**E. Optimize resource allocation**\n",
    "\n",
    "- **Adjust parallelism:** Set the number of partitions appropriately to utilize all available cores.\n",
    "  - Example:\n",
    "    ```python\n",
    "    num_partitions = sc.defaultParallelism * 2\n",
    "    clickstream_rdd = clickstream_rdd.repartition(num_partitions)\n",
    "    ```\n",
    "- **Executor Memory and Cores:** Configure executor memory and cores based on cluster's resources.\n",
    "\n",
    "**F. Data serialization**\n",
    "\n",
    "- **Use efficient serialization Formats:** Configure Spark to use Kryo serialization for faster data serialization.\n",
    "  - Example:\n",
    "    ```python\n",
    "    spark.conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    ```\n",
    "\n",
    "**G. Filter data early**\n",
    "\n",
    "- **Optimization:** Filter out unnecessary data as early as possible to reduce the amount of data processed downstream.\n",
    "\n",
    "**H. Monitor and tune performance**\n",
    "\n",
    "- **Use Spark UI:** Monitor job execution to identify bottlenecks.\n",
    "- **Adjust Configurations:** Tune Spark configurations like `spark.sql.shuffle.partitions` for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying algorithm to code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 11:31:09,564 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "[Stage 3:==========================================>              (24 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top routes from RDD approach using spark:\n",
      "{'main': 8185, 'main-archive': 1113, 'main-rabota': 1047, 'main-internet': 897, 'main-bonus': 870, 'main-news': 769, 'main-tariffs': 677, 'main-online': 587, 'main-vklad': 518, 'main-rabota-archive': 170}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Initialize spark context and session:\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"ClickstreamAnalysisRDD\").getOrCreate()\n",
    "\n",
    "# 2) define a function to parse CSV lines:\n",
    "def parse_csv_line(line):\n",
    "    \"\"\"\n",
    "    Parses a single CSV line into a structured dictionary.\n",
    "    \n",
    "    Args:\n",
    "        line (str): A line from the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with keys corresponding to CSV headers.\n",
    "    \"\"\"\n",
    "    fields = line.split('\\t')\n",
    "    if len(fields) != 5:\n",
    "        # malformed lines\n",
    "        return None\n",
    "    try:\n",
    "        return {\n",
    "            \"user_id\": int(fields[0]),\n",
    "            \"session_id\": int(fields[1]),\n",
    "            \"event_type\": fields[2],\n",
    "            \"event_page\": fields[3],\n",
    "            \"timestamp\": int(fields[4])\n",
    "        }\n",
    "    except ValueError:\n",
    "        # conversion errors\n",
    "        return None\n",
    "    \n",
    "# 3) Load data into an RDD: read the clickstream data from the HDFS path into an RDD.\n",
    "clickstream_rdd = sc.textFile(\"hdfs:/data/clickstream.csv\")\n",
    "\n",
    "# 4) Remove the header row:\n",
    "header = clickstream_rdd.first()\n",
    "clickstream_rdd = clickstream_rdd.filter(lambda line: line != header)\n",
    "\n",
    "# 5) parse csv lines into structured records\n",
    "parsed_rdd = clickstream_rdd.map(parse_csv_line).filter(lambda x: x is not None)\n",
    "\n",
    "# 6) key-value mapping and grouping (user_id, session_id)\n",
    "grouped_rdd = parsed_rdd.map(lambda row: ((row['user_id'], row['session_id']), row)) \\\n",
    "                        .groupByKey()\n",
    "\n",
    "# 7) function to construct user routes\n",
    "def construct_route(events):\n",
    "    \"\"\"\n",
    "    Constructs a navigation route from a sequence of events.\n",
    "    \n",
    "    Args:\n",
    "        events (Iterable[dict]): An iterable of dictionaries containing event data.\n",
    "        \n",
    "    Returns:\n",
    "        str: A hyphen-separated string representing the user's navigation route.\n",
    "    \"\"\"\n",
    "    # sort events by timestamp\n",
    "    sorted_events = sorted(events, key=lambda event: event['timestamp'])\n",
    "    \n",
    "    navigation_path = []\n",
    "    for event in sorted_events:\n",
    "        event_type = event['event_type'].lower()\n",
    "        event_page = event['event_page']\n",
    "        if \"error\" in event_type:\n",
    "            break\n",
    "        if event_type == \"page\":\n",
    "            if not navigation_path or navigation_path[-1] != event_page:\n",
    "                navigation_path.append(event_page)\n",
    "    \n",
    "    return \"-\".join(navigation_path)\n",
    "\n",
    "# 8) Construct routes for each session\n",
    "routes_rdd = grouped_rdd.mapValues(construct_route)\n",
    "\n",
    "# 9) filter out empty routes\n",
    "routes_rdd = routes_rdd.filter(lambda x: x[1] != \"\")\n",
    "\n",
    "# 10) map routes to (route, 1)\n",
    "route_counts_rdd = routes_rdd.map(lambda x: (x[1], 1))\n",
    "\n",
    "# 11) reduce by key to count occurrences\n",
    "final_counts_rdd = route_counts_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# 12) sort and take top 10\n",
    "top_routes = final_counts_rdd.takeOrdered(10, key=lambda x: -x[1])\n",
    "top_routes_dict = {route: count for route, count in top_routes}\n",
    "print(\"Top routes from RDD approach using spark:\")\n",
    "print(top_routes_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
