{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.2.4\n",
      "  Downloading pyspark-3.2.4.tar.gz (281.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.5/281.5 MB\u001b[0m \u001b[31m531.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m164.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.4-py2.py3-none-any.whl size=282040922 sha256=9dd24e8b850cf1696c0099f7d9a6488cfc85768c92cca089002fff7a5ef587e5\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/e7/e3/c8/c358dac750f2b6a4b03328d10e05a5c69501664bd6504b6c3e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==3.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DF-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n",
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|event_type  |event_page|timestamp |\n",
      "+-------+----------+------------+----------+----------+\n",
      "|562    |507       |page        |main      |1695584127|\n",
      "|562    |507       |event       |main      |1695584134|\n",
      "|562    |507       |event       |main      |1695584144|\n",
      "|562    |507       |event       |main      |1695584147|\n",
      "|562    |507       |wNaxLlerrorU|main      |1695584154|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def initialize_spark(app_name=\"ClickstreamAnalysisJSON\"):\n",
    "    \"\"\"Initializes and returns a SparkSession.\"\"\"\n",
    "    spark_context = SparkContext.getOrCreate()\n",
    "    spark_context.setLogLevel(\"WARN\")  # Set log level to WARN to reduce verbosity\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = initialize_spark()\n",
    "\n",
    "data_path = \"hdfs:/data/clickstream.csv\"\n",
    "\n",
    "# Load the clickstream data with tab delimiter and header\n",
    "clickstream_data = spark.read.csv(data_path, sep=\"\\t\", header=True)\n",
    "\n",
    "# schema to verify correct loading\n",
    "clickstream_data.printSchema()\n",
    "\n",
    "# sample of the data\n",
    "clickstream_data.show(5, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def construct_route(event_sequence):\n",
    "    \"\"\"\n",
    "    Constructs a user route from a sequence of events.\n",
    "    \n",
    "    Args:\n",
    "        event_sequence (list of tuples): Each tuple contains (event_type, event_page).\n",
    "        \n",
    "    Returns:\n",
    "        str: The constructed route as a hyphen-separated string.\n",
    "    \"\"\"\n",
    "    navigation = []\n",
    "    for event in event_sequence:\n",
    "        event_type, event_page = event\n",
    "        if \"error\" in event_type.lower():\n",
    "            break  # terminate route construction on encountering an error event\n",
    "        if event_type.lower() == \"page\":\n",
    "            if not navigation or navigation[-1] != event_page:\n",
    "                navigation.append(event_page)\n",
    "    return \"-\".join(navigation)\n",
    "\n",
    "# register the function as a UDF with StringType return\n",
    "route_construction_udf = udf(construct_route, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------------------------------------------------------------------+\n",
      "|user_id|session_id|route                                                                           |\n",
      "+-------+----------+--------------------------------------------------------------------------------+\n",
      "|1      |1026      |main                                                                            |\n",
      "|10     |762       |main-internet-archive-bonus-internet-main-rabota-main-archive-bonus-news-archive|\n",
      "|1002   |285       |main-news-internet-rabota-bonus                                                 |\n",
      "|1007   |82        |main-archive-digital                                                            |\n",
      "|1009   |639       |main-online-internet-digital-archive                                            |\n",
      "+-------+----------+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, collect_list, struct\n",
    "\n",
    "# a window specification partitioned by user and session, ordered by timestamp\n",
    "window_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
    "\n",
    "# aggregate event_type and event_page into a list per user-session\n",
    "aggregated_df = clickstream_data \\\n",
    "    .withColumn(\"event_struct\", struct(\"event_type\", \"event_page\")) \\\n",
    "    .withColumn(\"events\", collect_list(\"event_struct\").over(window_spec)) \\\n",
    "    .groupBy(\"user_id\", \"session_id\") \\\n",
    "    .agg(F.max(\"events\").alias(\"events\"))\n",
    "\n",
    "# apply the UDF to construct routes\n",
    "routes_df = aggregated_df.withColumn(\"route\", route_construction_udf(col(\"events\")))\n",
    "\n",
    "# the resulting DataFrame\n",
    "routes_df.select(\"user_id\", \"session_id\", \"route\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|route              |count|\n",
      "+-------------------+-----+\n",
      "|main               |8185 |\n",
      "|main-archive       |1113 |\n",
      "|main-rabota        |1047 |\n",
      "|main-internet      |897  |\n",
      "|main-bonus         |870  |\n",
      "|main-news          |769  |\n",
      "|main-tariffs       |677  |\n",
      "|main-online        |587  |\n",
      "|main-vklad         |518  |\n",
      "|main-rabota-archive|170  |\n",
      "+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:=================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"main\": 8185,\n",
      "    \"main-archive\": 1113,\n",
      "    \"main-rabota\": 1047,\n",
      "    \"main-internet\": 897,\n",
      "    \"main-bonus\": 870,\n",
      "    \"main-news\": 769,\n",
      "    \"main-tariffs\": 677,\n",
      "    \"main-online\": 587,\n",
      "    \"main-vklad\": 518,\n",
      "    \"main-rabota-archive\": 170\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# the frequency of each unique route\n",
    "route_frequencies = routes_df.groupBy(\"route\").count().orderBy(F.desc(\"count\"))\n",
    "\n",
    "# top 10 routes\n",
    "route_frequencies.show(10, truncate=False)\n",
    "top_n = 10\n",
    "top_routes = route_frequencies.limit(top_n).collect()\n",
    "\n",
    "routes_dict = {row['route']: row['count'] for row in top_routes}\n",
    "\n",
    "# output JSON file\n",
    "output_json_path = \"top10_routes.json\"\n",
    "with open(output_json_path, 'w') as json_file:\n",
    "    json.dump(routes_dict, json_file, indent=4)\n",
    "\n",
    "with open(output_json_path, 'r') as json_file:\n",
    "    print(json_file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "Correct main answer!\n",
      "Correct main-archive answer!\n",
      "Correct main-rabota answer!\n",
      "Correct main-internet answer!\n",
      "Correct main-bonus answer!\n",
      "Correct main-news answer!\n",
      "Correct main-tariffs answer!\n",
      "Correct main-online answer!\n",
      "Correct main-vklad answer!\n",
      "Correct main-rabota-archive answer!\n"
     ]
    }
   ],
   "source": [
    "!curl -F file=@top10_routes.json 51.250.123.136:80/MDS-LSML1/ntrusova/w6/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " # SQL-based approach\n",
    " ## 1. Define and Register a User-Defined Function (UDF) for Route Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 13:26:25,940 WARN analysis.SimpleFunctionRegistry: The function route_builder_udf replaced a previously registered function.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.build_user_route(event_records)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# a function to build user routes from event sequences\n",
    "def build_user_route(event_records):\n",
    "    \"\"\"\n",
    "    Constructs a navigation route for a user session based on event records.\n",
    "    \n",
    "    Args:\n",
    "        event_records (list of tuples): Each tuple contains (event_type, event_page, sequence_number).\n",
    "        \n",
    "    Returns:\n",
    "        str: A hyphen-separated string representing the user's navigation route.\n",
    "    \"\"\"\n",
    "    # sort events by their sequence number to ensure chronological order\n",
    "    sorted_events = sorted(event_records, key=lambda record: record[2])\n",
    "    \n",
    "    navigation_steps = []\n",
    "    for event_type, event_page, _ in sorted_events:\n",
    "        if \"error\" in event_type.lower():\n",
    "            break  # stop processing if an error\n",
    "        \n",
    "        if event_type.lower() == \"page\" and (not navigation_steps or navigation_steps[-1] != event_page):\n",
    "            navigation_steps.append(event_page)\n",
    "    \n",
    "    # a single route string\n",
    "    route = \"-\".join(navigation_steps)\n",
    "    return route\n",
    "\n",
    "# the Python function as a Spark UDF\n",
    "route_builder_udf = F.udf(build_user_route, StringType())\n",
    "spark.udf.register(\"route_builder_udf\", build_user_route, StringType())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Temporary View and Execute SQL Query to Generate Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------+\n",
      "|user_route                                                                      |\n",
      "+--------------------------------------------------------------------------------+\n",
      "|main                                                                            |\n",
      "|main-internet-archive-bonus-internet-main-rabota-main-archive-bonus-news-archive|\n",
      "|main-news-internet-rabota-bonus                                                 |\n",
      "|main-archive-digital                                                            |\n",
      "|main-online-internet-digital-archive                                            |\n",
      "+--------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# a temporary view for the clickstream data\n",
    "clickstream_data.createOrReplaceTempView(\"clickstream_data\")\n",
    "\n",
    "# SQL query to construct user routes using the registered UDF\n",
    "route_generation_query = \"\"\"\n",
    "    SELECT\n",
    "        route_builder_udf(collect_list(struct(event_type, event_page, seq_num))) AS user_route\n",
    "    FROM (\n",
    "        SELECT\n",
    "            user_id,\n",
    "            session_id,\n",
    "            event_type,\n",
    "            event_page,\n",
    "            ROW_NUMBER() OVER (PARTITION BY user_id, session_id ORDER BY timestamp) AS seq_num\n",
    "        FROM\n",
    "            clickstream_data\n",
    "    ) ordered_events\n",
    "    GROUP BY\n",
    "        user_id, session_id\n",
    "\"\"\"\n",
    "\n",
    "generated_routes_df = spark.sql(route_generation_query)\n",
    "\n",
    "generated_routes_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Aggregate and Count Unique Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+\n",
      "|route              |occurrence_count|\n",
      "+-------------------+----------------+\n",
      "|main               |8185            |\n",
      "|main-archive       |1113            |\n",
      "|main-rabota        |1047            |\n",
      "|main-internet      |897             |\n",
      "|main-bonus         |870             |\n",
      "|main-news          |769             |\n",
      "|main-tariffs       |677             |\n",
      "|main-online        |587             |\n",
      "|main-vklad         |518             |\n",
      "|main-rabota-archive|170             |\n",
      "+-------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# temporary view for the generated routes\n",
    "generated_routes_df.createOrReplaceTempView(\"routes_view\")\n",
    "\n",
    "# count the frequency of each unique route\n",
    "route_frequency_query = \"\"\"\n",
    "    SELECT\n",
    "        user_route AS route,\n",
    "        COUNT(*) AS occurrence_count\n",
    "    FROM\n",
    "        routes_view\n",
    "    GROUP BY\n",
    "        user_route\n",
    "    ORDER BY\n",
    "        occurrence_count DESC\n",
    "\"\"\"\n",
    "\n",
    "route_frequencies_df = spark.sql(route_frequency_query)\n",
    "\n",
    "# top 10 most frequent routes\n",
    "route_frequencies_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Top Routes and Export to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:=================================================>      (16 + 2) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"main\": 8185,\n",
      "    \"main-archive\": 1113,\n",
      "    \"main-rabota\": 1047,\n",
      "    \"main-internet\": 897,\n",
      "    \"main-bonus\": 870,\n",
      "    \"main-news\": 769,\n",
      "    \"main-tariffs\": 677,\n",
      "    \"main-online\": 587,\n",
      "    \"main-vklad\": 518,\n",
      "    \"main-rabota-archive\": 170\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "top_n_routes = 10\n",
    "\n",
    "# collect the top N routes into a list of Row objects\n",
    "top_routes = route_frequencies_df.limit(top_n_routes).collect()\n",
    "\n",
    "top_routes_dict = {row['route']: row['occurrence_count'] for row in top_routes}\n",
    "\n",
    "output_json_file = \"top10_routes_sql.json\"\n",
    "\n",
    "with open(output_json_file, 'w') as json_file:\n",
    "    json.dump(top_routes_dict, json_file, indent=4)\n",
    "\n",
    "with open(output_json_file, 'r') as json_file:\n",
    "    print(json_file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "clickstream_rdd = clickstream_data.rdd\n",
    "\n",
    "# extract relevant event information\n",
    "def extract_event(row):\n",
    "    \"\"\"\n",
    "    Extracts user ID, session ID, event type, event page, and timestamp from a row.\n",
    "    \n",
    "    Args:\n",
    "        row (Row): A Spark Row object containing clickstream data.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing ((user_id, session_id), (event_type, event_page, timestamp))\n",
    "    \"\"\"\n",
    "    return ((row['user_id'], row['session_id']),\n",
    "            (row['event_type'], row['event_page'], row['timestamp']))\n",
    "\n",
    "# group events by user_id and session_id\n",
    "grouped_events_rdd = clickstream_rdd.map(extract_event) \\\n",
    "                                   .groupByKey()\n",
    "\n",
    "# construct routes from grouped events\n",
    "def construct_route(events):\n",
    "    \"\"\"\n",
    "    Constructs a navigation route from a sequence of events.\n",
    "    \n",
    "    Args:\n",
    "        events (Iterable[tuple]): An iterable of tuples containing (event_type, event_page, timestamp).\n",
    "        \n",
    "    Returns:\n",
    "        str: A hyphen-separated string representing the user's navigation route.\n",
    "    \"\"\"\n",
    "    # sort based on timestamp to ensure chronological order\n",
    "    sorted_events = sorted(events, key=lambda event: event[2])\n",
    "    \n",
    "    navigation_path = []\n",
    "    for event_type, event_page, _ in sorted_events:\n",
    "        if \"error\" in event_type.lower():\n",
    "            break  # stop processing if an error event is encountered\n",
    "        if event_type.lower() == \"page\":\n",
    "            if not navigation_path or navigation_path[-1] != event_page:\n",
    "                navigation_path.append(event_page)\n",
    "    \n",
    "    return \"-\".join(navigation_path)\n",
    "\n",
    "# apply the route construction function to each group\n",
    "routes_rdd = grouped_events_rdd.mapValues(construct_route)\n",
    "\n",
    "# count the occurrences of each unique route\n",
    "route_counts_rdd = routes_rdd.map(lambda route: (route[1], 1)) \\\n",
    "                             .reduceByKey(lambda count1, count2: count1 + count2)\n",
    "\n",
    "# sort the routes by their count in descending order\n",
    "sorted_route_counts_rdd = route_counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# collect the sorted route counts to the driver for further processing\n",
    "collected_route_counts = sorted_route_counts_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Routes Dictionary from RDD Approach:\n",
      "{'main': 8185, 'main-archive': 1113, 'main-rabota': 1047, 'main-internet': 897, 'main-bonus': 870, 'main-news': 769, 'main-tariffs': 677, 'main-online': 587, 'main-vklad': 518, 'main-rabota-archive': 170}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'main': 8185,\n",
       " 'main-archive': 1113,\n",
       " 'main-rabota': 1047,\n",
       " 'main-internet': 897,\n",
       " 'main-bonus': 870,\n",
       " 'main-news': 769,\n",
       " 'main-tariffs': 677,\n",
       " 'main-online': 587,\n",
       " 'main-vklad': 518,\n",
       " 'main-rabota-archive': 170}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 10\n",
    "\n",
    "top_routes_rdd = collected_route_counts[:top_n]\n",
    "top_routes_dict = {route: count for route, count in top_routes_rdd}\n",
    "\n",
    "print(\"Top Routes Dictionary from RDD Approach:\")\n",
    "print(top_routes_dict)\n",
    "\n",
    "top_routes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
